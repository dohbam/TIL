# 2. Linear Regression 이해하기

> #### **용어 정리**
>
> - **선형 회귀 (Linear Regression):** **지도학습**의 방법 중 regression에 해당되는 알고리즘. 종속 변수 y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 회귀분석 기법입니다. ([위키링크](https://ko.wikipedia.org/wiki/선형_회귀))
> - **다항식 회귀 (Polynomial Regression):**degree(차수)가 높은 regression 모델. 복잡도가 더 높으며 Overfit 될 수 있다는 것이 특징입니다. ([위키링크](https://en.wikipedia.org/wiki/Polynomial_regression))
> - **잔차제곱합/RSS (Residual Sum of Square):**예측된 Y값과 실제 Y값의 차이를 줄이는 Weight(가중치)를 찾는 방법입니다. ([위키링크](https://en.wikipedia.org/wiki/Residual_sum_of_squares))
> - **리지 회귀분석 (Ridge Regression):** Linear Regression에 L2 regularization을 사용하는 방법으로 모델의 복잡도를 줄여서 좀 더 간단하고 부드러운 모델로 만들때 사용합니다.
> - **정규화 (Regularization):** W(weight)가 너무 큰 값들을 갖지 않도록 하여 모델의 복잡도를 낮추는 방법입니다. ([위키링크](https://en.wikipedia.org/wiki/Regularization_(mathematics)))

### 강의

* Linear Regression
  * line fitting, 주어진 x에 대해 y값을 **실수**로 예측하는 것
  * 선형이 아닌 non-linear relationship도 모델링 가능
  * 예) Polynomial Regression : x, x^2, x^3 형식의
  * Multivariate linear regression : x1, x2 ...
  * Maximum likelihood estimation : w 찾기 (계수와 상수)
  * Residual Sum of Squares(RSS)
    * 실제 y값과 예측한 값의 차이의 제곱의 합을 최소화
  * Ridge Regression
    * MLE can overfit
    * Regularization, 불필요하게 복잡한 부분을 쳐내기 위해 람다*w^2를 추가해 패널티를 주는 게 Ridge Regression
  * Regularization
    * 모든 머신러닝 모델에서 중요.
    * w^2해서 패널티 주는 방법 : l2 norm, l2 reg.
    * big data - model이 자연스럽게 reg.
    * overfit 완화
  * overfitting 시 - 기존 데이터에 아주 잘 맞지만 이 때문에 오히려 새로운 데이터를 제대로 예측하지 못함

### 실습

* 1) 기울기와 절편

  > #### 기울기와 절편
  >
  > 단순 선형회귀 분석 수식은 다음과 같습니다.
  >
  > y^{(i)} \sim \beta_0 x^{(i)} + \beta_1*y*(*i*)∼*β*0*x*(*i*)+*β*1
  >
  > 여기서 \beta_0*β*0 은 기울기, \beta_1*β*1 은 y*y*절편을 의미합니다.
  >
  > 코드를 실행하여 기울기와 y*y*절편이 의미하는 것을 이해하여 봅니다.
  >
  > #### 실습
  >
  > 1. 코드를 실행해보고 결과를 확인해보세요.
  > 2. 베타 값을 수정하면 그래프가 어떻게 변하는지 실행하여 확인해 봅시다.

  ```python
  # 실습에 필요한 패키지입니다. 수정하지 마세요.
  import elice_utils
  import matplotlib as mpl
  mpl.use("Agg")
  import matplotlib.pyplot as plt
  import numpy as np
  eu = elice_utils.EliceUtils()
  
  # 실습에 필요한 데이터입니다. 수정하지마세요. 
  X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513, 5.48321616]
  Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441, 5.19692852]
  
  '''
  beta_0과 beta_1 을 변경하면서 그래프에 표시되는 선을 확인해 봅니다.
  기울기와 절편의 의미를 이해합니다.
  '''
  
  beta_0 = 0.7   # beta_0에 저장된 기울기 값을 조정해보세요. 
  beta_1 = 1.2 # beta_1에 저장된 절편 값을 조정해보세요.
  
  plt.scatter(X, Y) # (x, y) 점을 그립니다.
  plt.plot([0, 10], [beta_1, 10 * beta_0 + beta_1], c='r') # y = beta_0 * x + beta_1 에 해당하는 선을 그립니다.
  
  plt.xlim(0, 10) # 그래프의 X축을 설정합니다.
  plt.ylim(0, 10) # 그래프의 Y축을 설정합니다.
  
  # 엘리스에 이미지를 표시합니다.
  plt.savefig("test.png")
  eu.send_image("test.png")
  ```

* 2) 손실 함수(Loss function)

  > #### Loss Function
  >
  > 앞서 배운 선형 회귀분석 모델에서 Loss Function을 구하는 방법을 알아보겠습니다.
  >
  > y^{(i)} \sim \beta_0 x^{(i)} + \beta_1*y*(*i*)∼*β*0*x*(*i*)+*β*1
  >
  > Loss function 은 예측한 데이터와 실제 데이터와의 차이로 다음과 같이 정의 할 수 있습니다.
  >
  > Loss =mean( (y-y_{predict})^{2})*L**o**s**s*=*m**e**a**n*((*y*−*y**p**r**e**d**i**c**t*)2)
  >
  > #### 실습
  >
  > 1. loss(x, y, beta_0, beta_1) 함수를 완성합니다.
  > 2. beta_0, beta_1 값을 바꿔가며 loss값을 출력해봅니다.
  > 3. loss값이 줄어들었다면 그래프 상으로 어떤 변화가 있었는지 확인합니다.
  > 4. loss값이 최소값이 되도록 beta_0, beta_1값을 찾는 좋은 방식이 있을까 생각해봅니다.

  ```python
  # 원본 코드
  import elice_utils
  import matplotlib as mpl
  mpl.use("Agg")
  import matplotlib.pyplot as plt
  import numpy as np
  eu = elice_utils.EliceUtils()
  
  def loss(x, y, beta_0, beta_1):
      N = len(x)
      
      '''
      x, y, beta_0, beta_1 을 이용해 loss값을 계산한 뒤 리턴합니다.
      '''
      np_x = np.array(x)
      np_y = np.array(y)
      y_predict = np_x * beta_0 + beta_1
      
  
      return np.sum((y - y_predict)**2)
  
  X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513, 5.48321616]
  Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441, 5.19692852]
  
  beta_0 = None # 기울기
  beta_1 = None # 절편
  
  print("Loss: %f" % loss(X, Y, beta_0, beta_1))
  
  plt.scatter(X, Y) # (x, y) 점을 그립니다.
  plt.plot([0, 10], [beta_1, 10 * beta_0 + beta_1], c='r') # y = beta_0 * x + beta_1 에 해당하는 선을 그립니다.
  
  plt.xlim(0, 10) # 그래프의 X축을 설정합니다.
  plt.ylim(0, 10) # 그래프의 Y축을 설정합니다.
  plt.savefig("test.png") # 저장 후 엘리스에 이미지를 표시합니다.
  eu.send_image("test.png")
  ```

* 3) Scikit-learn을 이용한 linear regression

  > #### Scikit-learn을 이용한 linear regression
  >
  > 기계학습 라이브러리 Scikit-learn을 사용하면 최적화 된 \beta_0*β*0, \beta_1*β*1 을 쉽게 구할 수 있습니다.
  >
  > 주어진 데이터와 다음 선형 모델을 이용하여 최적의 \beta_0*β*0, \beta_1*β*1 값을 [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) 라이브러리를 사용하여 구할 수 있습니다.
  >
  > y^{(i)} \sim \beta_0 x^{(i)} + \beta_1*y*(*i*)∼*β*0*x*(*i*)+*β*1
  >
  > #### 실습
  >
  > 1. `np.array(X).reshape(-1, 1)` 명령어를 이용해 길이 10인 1차원 리스트 `X` 를 10\times 110×1 형태의 `np.array`로 변경하세요.
  >
  > 2. 리스트 `Y`를 `np.array` 형식으로 변경하세요.
  >
  > 3. 모델을 학습
  >
  >    ```
  >    lrmodel = LinearRegression()
  >    lrmodel.fit(train_X, train_Y)
  >    Copy
  >    ```
  >
  > 4. 모델을 이용해 얻은 최적의 `beta_0`, `beta_1`값을 확인합니다.

  ```python
  # 원본 코드
  import matplotlib as mpl
  mpl.use("Agg")
  import matplotlib.pyplot as plt
  import numpy as np
  from sklearn.linear_model import LinearRegression
  
  import elice_utils
  eu = elice_utils.EliceUtils()
  
      
  X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513, 5.48321616]
  Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441, 5.19692852]
  
  train_X = ____________
  train_Y = ____________
  
  '''
  여기에서 모델을 트레이닝합니다.
  '''
  lrmodel = ________________
  lrmodel.fit(_______, _______)
  
  beta_0 = lrmodel.coef_[0]
  beta_1 = lrmodel.intercept_
  
  print("beta_0: %f" % beta_0)
  print("beta_1: %f" % beta_1)
  
  plt.scatter(X, Y) # (x, y) 점을 그립니다.
  plt.plot([0, 10], [beta_1, 10 * beta_0 + beta_1], c='r') # y = beta_0 * x + beta_1 에 해당하는 선을 그립니다.
  
  plt.xlim(0, 10) # 그래프의 X축을 설정합니다.
  plt.ylim(0, 10) # 그래프의 Y축을 설정합니다.
  plt.savefig("test.png") # 저장 후 엘리스에 이미지를 표시합니다.
  eu.send_image("test.png")
  ```

* 4) 선형 회귀 구현하기

  > #### 선형 회귀 구현하기
  >
  > 선형 회귀는 종속 변수 y와 한 개 이상의 독립 변수 X와의 선형 상관 관계를 모델링하는 회귀분석 기법을 말한다.
  >
  > 이번 시간에는 y와 x가 주어졌을 때, ‘y = ax+b’ 라는 형태의 직선을 회귀식으로 하는 단순한 선형 회귀(Linear Regression) 파이썬을 통해 직접 구현해보도록 하자.
  >
  > ------
  >
  > **선형 회귀의 절차**
  >
  > 1. x라는 값이 입력되면 'ax+b’라는 계산식을 통해 값을 산출하는 예측 함수를 정의한다.
  > 2. 예측 함수를 통해 예측값과 실제값 y 간의 차이를 계산한다.
  > 3. a와 b를 업데이트 하는 규칙을 정의하고 이를 바탕으로 a와 b의 값을 조정한다.
  > 4. 위의 과정을 특정 반복횟수 만큼 반복한다.
  > 5. 반복적으로 수정된 a와 b를 바탕으로 ‘y=ax+b’ 라는 회귀식을 정의한다.
  >
  > ------
  >
  > #### 실습
  >
  > 1. 학습률을 직접 설정해 본다.
  > 2. 반복횟수를 설정한다.
  > 3. Numpy 배열 a, b, x 를 받아서 'ax+b’를 계산하는 prediction 함수를 정의한다.
  > 4. 실제값과 예측값의 차이를 계산하여 error를 정의한다.
  > 5. matplotlib을 사용해 그래프를 그려보고 위 설정 값에 따라 회귀 직선이 어떻게 변화하는지 살펴보자.

  ```python
  # 원본 코드
  import numpy as np
  import elice_utils
  import matplotlib.pyplot as plt
  import matplotlib as mpl
  mpl.use("Agg")
  eu = elice_utils.EliceUtils()
  #학습률(learning rate)를 설정한다.(권장: 0.0001~0.01)
  learning_rate = None 
  #반복 횟수(iteration)를 설정한다.(자연수)
  iteration = None
  def prediction(a,b,x):
      # 넘파이 배열 a,b,x를 받아서 'x*(transposed)a + b'를 계산하는 식을 만든다.
      equation = None
      
      return equation
      
  def update_ab(a,b,x,error,lr):
      # a를 업데이트하는 규칙을 정의한다.
      delta_a = -(lr*(2/len(error))*(np.dot(x.T, error)))
      # b를 업데이트하는 규칙을 정의한다.
      delta_b = -(lr*(2/len(error))*np.sum(error))
      
      return delta_a, delta_b
      
  def gradient_descent(x, y, iters):
      #초기값 a= 0, a=0
      a = np.zeros((1,1))
      b = np.zeros((1,1))
      
      for i in range(iters):
          #실제 값 y와 예측 값의 차이를 계산하여 error를 정의한다.
          error = None
          a_delta, b_delta = update_ab(a,b,x,error,lr=learning_rate)
          a -= a_delta
          b -= b_delta
          
      return a, b
  
  def plotting_graph(x,y,a,b):
      y_pred=a[0,0]*x+b
      plt.scatter(x, y)
      plt.plot(x, y_pred)
      plt.savefig("test.png")
      eu.send_image("test.png")
  
  def main():
  
      x = 5*np.random.rand(100,1)
      y = 3*x + 5*np.random.rand(100,1)
      
      a, b = gradient_descent(x,y,iters=iteration)
      
      print("a:",a, "b:",b)
      plotting_graph(x,y,a,b)
      
  main()
  ```

* 5) 릿지 회귀(Ridge Regression) 구현하기

  > #### 릿지 회귀(Ridge Regression) 구현하기
  >
  > 릿지 회귀는 일반적인 선형회귀에서 L2 규제 항(regularization terms)이 추가된 회귀를 의미합니다.
  >
  > 이번 시간에는 릿지 회귀를 직접구현해보고, 파라미터를 변경해가며 회귀 결과가 어떻게 변화하는지 살펴봅니다.
  >
  > ------
  >
  > 1. x라는 값이 입력되면 'ax+b’라는 계산식을 통해 값을 산출하는 예측 함수를 정의합니다.
  > 2. 예측 함수를 통해 예측값과 실제값 y 간의 차이를 계산합니다.
  > 3. a와 b를 업데이트 하는 규칙을 정의하고 이를 바탕으로 a와 b의 값을 조정합니다. (alpha 값을 이용하여 규제 항을 설정합니다.)
  > 4. 위의 과정을 특정 반복횟수 만큼 반복합니다.
  > 5. 반복적으로 수정된 a와 b를 바탕으로 ‘y=ax+b’ 라는 회귀식을 정의합니다.
  >
  > #### 실습
  >
  > 1. 학습률을 직접 설정해 봅니다.
  > 2. 반복횟수를 설정합니다.
  > 3. alpha 값을 설정합니다.
  > 4. Numpy 배열 a, b, x 를 받아서 'ax+b’를 계산하는 prediction 함수를 정의합니다.
  > 5. 규제항을 alpha값과 a의 곱으로 설정하고 a와 b를 업데이트하는 규칙에 이를 추가합니다.
  > 6. 실제값과 예측값의 차이를 계산하여 error를 정의합니다.
  > 7. matplotlib을 사용해 그래프를 그려보고 위 설정 값에 따라 회귀 직선이 어떻게 변화하는지 살펴봅니다.

  ```python
  import numpy as np
  import elice_utils
  import matplotlib.pyplot as plt
  import matplotlib as mpl
  mpl.use("Agg")
  eu = elice_utils.EliceUtils()
  
  #학습률(learning rate)를 설정한다.(권장: 0.0001~0.01)
  learning_rate = None
  #반복 횟수(iteration)를 설정한다.(자연수)
  iteration = None
  #릿지회귀에 사용되는 알파(alpha) 값을 설정한다.(권장: 0.0001~0.01)
  alpha = None
  
  def prediction(a,b,x):    
      return None
      
  def update_ab(a,b,x,error,lr, alpha):
      #alpha와 a의 곱으로 regularization을 설정한다.  
      regularization = None
      delta_a = -(lr*(2/len(error))*(np.dot(x.T, error)) + regularization)
      delta_b = -(lr*(2/len(error))*np.sum(error))
      return delta_a, delta_b
      
  def gradient_descent(x, y, iters, alpha):
      #초기값 a= 0, a=0
      a = np.zeros((1,1))
      b = np.zeros((1,1))    
      
      for i in range(iters):
          error = None
          a_delta, b_delta = update_ab(a,b,x,error,lr=learning_rate, alpha=alpha)
          a -= a_delta
          b -= b_delta
      
      return a, b
  
  def plotting_graph(x,y,a,b):
      y_pred=a[0,0]*x+b
      plt.scatter(x, y)
      plt.plot(x, y_pred)
      plt.savefig("test.png")
      eu.send_image("test.png")
  
  def main():
      #x, y 데이터를 생성한다.
      x = 5*np.random.rand(100,1)
      y = 10*x**4 + 2*x + 1+ 5*np.random.rand(100,1)
      # a와 b의 값을 반복횟수만큼 업데이트하고 그 값을 출력한다. 
      a, b = gradient_descent(x,y,iters=iteration, alpha=alpha)
      print("a:",a, "b:",b)
      #회귀 직선과 x,y의 값을 matplotlib을 통해 나타낸다.
      plotting_graph(x,y,a,b)
      
  main()
  ```

### 해설

* 기계 학습 (Machine Learning)
  * unsupervised, 비지도학습, data
  * supervised, 지도학습, data with tag
    * regression
      * y = ax + b 에서 실제값과 예측값의 차이를 최소화는 a와 b를 구하기
    * clustering
  * reinforcement, 강화학습, reword maximize
  * 모든 기계학습은 target function의 maximize가 목표

* Linear Regression

  * target function : - loss function
  * loss function : 선과 데이터 값 차이의 제곱의 합
  * 제곱을 하는 이유 : 음수차와 양수차 모두 차이인데 그냥 더하면 값이 제대로 나오지 않음. 절대값 또는 제곱으로 해결할 수 있는데 절대값은 미분이 어려워서 제곱 사용.
  * target function에서 뾰족한 함수는 잘 쓰지 않음.
    * 예) 시그모이드 함수 등을 사용. 부드럽고 둥글게 생겼음.
  * Gradient Descent
    * loss function이 최소인 w를 찾기 위해 w로 미분. 가장 낮은 포인트를 향해 점점 미끄러져 내려가는 걸 의미.
    * 각각의 파라미터에 대해 미분 후 슬슬 찾아가는 방식.
    * 딥러닝에서 파라미터를 찾는 것도 개념적으론 이것과 비슷. 파라미터가 엄청 많지만. 
  * Overfit
    * target function이 높다고 무조건 좋은 건 아님
    * data에는 항상 노이즈가 있음.
    * 과적합하면 오히려 성능이 떨어짐.
  * Ridge Regression
    * 과적합하지 않기 위한 패널티 부여
    * 딥러닝에서 많이 사용.

* 1) 기울기와 절편

  * 최적값을 알아내고 싶은 것
  * 실제값은 0.5에 2, 노이즈 조금 추가한 dataset

* 2) Loss Function

  ```python
  np_x = np.array(x)
  np_y = np.array(y)
  y_predict = np_x * beta_0 + beta_1
  np.sum((y - y_predict)**2) # loss, 실습2 return값
  ```

* 3) Scikit-learn을 이용한 linear regression

  > linear regression gradient descent로 검색해보면 참고 자료 많이 볼 수 있음.

  * np.array(X).reshape(-1, 1)
    * -1 : 모르니까 알아서 해달라고 넣는 값
  * 결과 : 0.43에 2.5 정도 나옴. 노이즈 때문.

* Q) 5번 코드에서 전치행렬을 쓰는 이유는?

  ```text
  A) y = ax^2 + bx + c 라는 모델이 있다고 생각해보자.
  x = 2 일 때, y = 4a + 2b + c
  [a b c].[(세로) x^2 x 1]
  이거 좌우를 바꾸어서
  [x^2 x 1].[(세로) a b c]로 한 것뿐
  ```

  

